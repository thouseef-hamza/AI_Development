{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangSmith Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith is an advanced open-source library tailored for AI Engineers, focusing on enabling seamless development and deployment of language-based AI systems. With an emphasis on modularity and interoperability, LangSmith simplifies the integration of natural language processing (NLP) capabilities into AI workflows, offering intuitive tools for fine-tuning, optimization, and interaction. It empowers developers to switch effortlessly between language models, customize pipelines, and deploy scalable, production-ready applications with minimal friction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super important note, you need to run LangSmith on more then 0.1.45 and be using the current API keys and not the legacy keys, if you do not do this you will recieve a 401 error message saying the token you are using is invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below is the enviroment data for enabling tracing, very important that you do not mess around with this as even replacing a singular character can break and then you will not be able to trace anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangSmith API Key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"aai-langchain-course-langsmith-starter-ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will be using OpenAI, however this is basically the exact same for llama just with a few changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2:1b-instruct-fp16\"\n",
    "\n",
    "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langsmith by default will trace all the usual langchain based LLM calls, which in the tutorials case is super useful as we won't have to edit much code to get traces working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2024-12-31T09:09:42.4472847Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 24838534100, 'load_duration': 19959371300, 'prompt_eval_count': 26, 'prompt_eval_duration': 499363000, 'eval_count': 18, 'eval_duration': 4345925000}, id='run-43697eeb-1932-40e5-9e76-db2f218f2796-0', usage_metadata={'input_tokens': 26, 'output_tokens': 18, 'total_tokens': 44})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is settup for the non-langchain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use a non langchain related function, and these are not automatically traced by langsmith, so instead, we have to add the traceable decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str):\n",
    "\n",
    "    complete_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a happy assistant\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    return client.chat(\n",
    "        model = model_name,\n",
    "        messages = complete_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2:1b-instruct-fp16',\n",
       " 'created_at': '2024-12-31T09:15:20.0684098Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I'm doing great, thanks for asking! I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always ready to help and assist you with any questions or tasks you may have. It's nice to have a conversation with someone new and interesting! How about you? How's your day going so far?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 35365044500,\n",
       " 'load_duration': 13020718600,\n",
       " 'prompt_eval_count': 35,\n",
       " 'prompt_eval_duration': 617363000,\n",
       " 'eval_count': 73,\n",
       " 'eval_duration': 21693342000}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add data into the traceable by editing its paramaters, this includes metadata, and what we will change, the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"OpenAI Response\")\n",
    "def generate_second_response(question: str):\n",
    "\n",
    "    complete_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a happy assistant\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    return client.chat(\n",
    "        model = model_name,\n",
    "        messages = complete_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3.2:1b-instruct-fp16',\n",
       " 'created_at': '2024-12-31T09:16:13.7821909Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I'm doing great, thanks for asking! I'm a large language model, so I don't have emotions or feelings like humans do, but I'm always ready to help and assist with any questions or tasks you may have. It's nice to start the day on a positive note, isn't it? How about you, how's your day going so far?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 26864340500,\n",
       " 'load_duration': 98078200,\n",
       " 'prompt_eval_count': 35,\n",
       " 'prompt_eval_duration': 890598000,\n",
       " 'eval_count': 75,\n",
       " 'eval_duration': 25866451000}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_second_response(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can view all of this over at the LangSmith website, under the tracing projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
